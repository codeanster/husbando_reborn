{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "anime_face_detection.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyMSJ3VyYDb7b/C6H0S+UaEv",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/codeanster/husbando_reborn/blob/main/anime_face_detection.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qSJOILrTCjFv"
      },
      "outputs": [],
      "source": [
        "#install dependencies |  make sure to run this with GPU\n",
        "!pip3 install opencv-contrib-python\n",
        "!pip3 install opencv-python-headless==4.5.2.52\n",
        "!pip install openmim\n",
        "!mim install mmcv-full\n",
        "!mim install mmdet\n",
        "!pip3 install anime-face-detector"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#import dependencies\n",
        "import cv2\n",
        "import numpy as np\n",
        "from anime_face_detector import create_detector\n",
        "import matplotlib.pyplot as plt\n",
        "from PIL import Image as im\n",
        "\n",
        "#download detector, open image and detect faces\n",
        "detector = create_detector('yolov3')\n",
        "image = cv2.imread('/content/1117465.png')\n",
        "preds = detector(image)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "osOMmg1tDWwD",
        "outputId": "a490dc8d-3294-4168-9256-8ddfb6ad31ba"
      },
      "execution_count": 66,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "load checkpoint from local path: /root/.cache/torch/hub/checkpoints/mmpose_anime-face_hrnetv2.pth\n",
            "load checkpoint from local path: /root/.cache/torch/hub/checkpoints/mmdet_anime-face_yolov3.pth\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/mmdet/datasets/utils.py:70: UserWarning: \"ImageToTensor\" pipeline is replaced by \"DefaultFormatBundle\" for batch inference. It is recommended to manually replace it in the test data pipeline in your config file.\n",
            "  'data pipeline in your config file.', UserWarning)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Contour Definition\n",
        "\n",
        "# https://github.com/hysts/anime-face-detector/blob/main/assets/landmarks.jpg\n",
        "FACE_BOTTOM_OUTLINE = np.arange(0, 5)\n",
        "LEFT_EYEBROW = np.arange(5, 8)\n",
        "RIGHT_EYEBROW = np.arange(8, 11)\n",
        "LEFT_EYE_TOP = np.arange(11, 14)\n",
        "LEFT_EYE_BOTTOM = np.arange(14, 17)\n",
        "RIGHT_EYE_TOP = np.arange(17, 20)\n",
        "RIGHT_EYE_BOTTOM = np.arange(20, 23)\n",
        "NOSE = np.array([23])\n",
        "MOUTH_OUTLINE = np.arange(24, 28)\n",
        "\n",
        "FACE_OUTLINE_LIST = [FACE_BOTTOM_OUTLINE, LEFT_EYEBROW, RIGHT_EYEBROW]\n",
        "LEFT_EYE_LIST = [LEFT_EYE_TOP, LEFT_EYE_BOTTOM]\n",
        "RIGHT_EYE_LIST = [RIGHT_EYE_TOP, RIGHT_EYE_BOTTOM]\n",
        "NOSE_LIST = [NOSE]\n",
        "MOUTH_OUTLINE_LIST = [MOUTH_OUTLINE]\n",
        "\n",
        "# (indices, BGR color, is_closed)\n",
        "CONTOURS = [\n",
        "    (FACE_OUTLINE_LIST, (0, 170, 255), False),\n",
        "    (LEFT_EYE_LIST, (50, 220, 255), False),\n",
        "    (RIGHT_EYE_LIST, (50, 220, 255), False),\n",
        "    (NOSE_LIST, (255, 30, 30), False),\n",
        "    (MOUTH_OUTLINE_LIST, (255, 30, 30), True),\n",
        "]"
      ],
      "metadata": {
        "id": "PV-tkVe7Gu-2"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Visualization Function\n",
        "\n",
        "\n",
        "def visualize_box(image,\n",
        "                  box,\n",
        "                  score,\n",
        "                  lt,\n",
        "                  box_color=(0, 255, 0),\n",
        "                  text_color=(255, 255, 255),\n",
        "                  show_box_score=True):\n",
        "    cv2.rectangle(image, tuple(box[:2]), tuple(box[2:]), box_color, lt)\n",
        "    if not show_box_score:\n",
        "        return\n",
        "    cv2.putText(image,\n",
        "                f'{round(score * 100, 2)}%', (box[0], box[1] - 2),\n",
        "                0,\n",
        "                lt / 2,\n",
        "                text_color,\n",
        "                thickness=max(lt, 1),\n",
        "                lineType=cv2.LINE_AA)\n",
        "\n",
        "\n",
        "def visualize_landmarks(image, pts, lt, landmark_score_threshold):\n",
        "    for *pt, score in pts:\n",
        "        pt = tuple(np.round(pt).astype(int))\n",
        "        if score < landmark_score_threshold:\n",
        "            color = (0, 255, 255)\n",
        "        else:\n",
        "            color = (0, 0, 255)\n",
        "        cv2.circle(image, pt, lt, color, cv2.FILLED)\n",
        "\n",
        "\n",
        "def draw_polyline(image, pts, color, closed, lt, skip_contour_with_low_score,\n",
        "                  score_threshold):\n",
        "    if skip_contour_with_low_score and (pts[:, 2] < score_threshold).any():\n",
        "        return\n",
        "    pts = np.round(pts[:, :2]).astype(int)\n",
        "    cv2.polylines(image, np.array([pts], dtype=np.int32), closed, color, lt)\n",
        "\n",
        "\n",
        "def visualize_contour(image, pts, lt, skip_contour_with_low_score,\n",
        "                      score_threshold):\n",
        "    for indices_list, color, closed in CONTOURS:\n",
        "        for indices in indices_list:\n",
        "            draw_polyline(image, pts[indices], color, closed, lt,\n",
        "                          skip_contour_with_low_score, score_threshold)\n",
        "\n",
        "\n",
        "def visualize(image: np.ndarray,\n",
        "              preds: np.ndarray,\n",
        "              face_score_threshold: float,\n",
        "              landmark_score_threshold: float,\n",
        "              show_box_score: bool = True,\n",
        "              draw_contour: bool = True,\n",
        "              skip_contour_with_low_score=False):\n",
        "    res = image.copy()\n",
        "\n",
        "    for pred in preds:\n",
        "        box = pred['bbox']\n",
        "        box, score = box[:4], box[4]\n",
        "        box = np.round(box).astype(int)\n",
        "        pred_pts = pred['keypoints']\n",
        "\n",
        "        # line_thickness\n",
        "        lt = max(2, int(3 * (box[2:] - box[:2]).max() / 256))\n",
        "\n",
        "        visualize_box(res, box, score, lt, show_box_score=show_box_score)\n",
        "        if draw_contour:\n",
        "            visualize_contour(\n",
        "                res,\n",
        "                pred_pts,\n",
        "                lt,\n",
        "                skip_contour_with_low_score=skip_contour_with_low_score,\n",
        "                score_threshold=landmark_score_threshold)\n",
        "        visualize_landmarks(res, pred_pts, lt, landmark_score_threshold)\n",
        "\n",
        "    return res"
      ],
      "metadata": {
        "cellView": "form",
        "id": "T72nSvd5F9y8"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Visualization Arguments\n",
        "\n",
        "face_score_threshold = 0  #@param {type: 'slider', min: 0, max: 1, step:0.1}\n",
        "landmark_score_threshold = 0  #@param {type: 'slider', min: 0, max: 1, step:0.1}\n",
        "show_box_score = True  #@param {'type': 'boolean'}\n",
        "draw_contour = True  #@param {'type': 'boolean'}\n",
        "skip_contour_with_low_score = True  #@param {'type': 'boolean'}"
      ],
      "metadata": {
        "cellView": "form",
        "id": "5YK1efFBF1mf"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "res = visualize(image, preds, face_score_threshold, landmark_score_threshold,\n",
        "                show_box_score, draw_contour, skip_contour_with_low_score)\n",
        "\n",
        "plt.figure(figsize=(30, 30))\n",
        "plt.imshow(res[:, :, ::-1])\n",
        "plt.axis('off')\n",
        "plt.show()"
      ],
      "metadata": {
        "cellView": "code",
        "id": "sNGr4tDPEjtx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#get predictions for face detection algorithm\n",
        "pred = preds[0]\n",
        "box = pred['bbox']\n",
        "box, score = box[:4], box[4]\n",
        "box = np.round(box).astype(int)\n",
        "pred_pts = pred['keypoints']\n",
        "\n",
        "#cropping image\n",
        "extra_room = int(image.shape[1] * .08)\n",
        "cropped_image = image[box[1]- extra_room:box[0] + extra_room,box[3] - extra_room:box[2]+extra_room]\n",
        "\n",
        "#save cropped image\n",
        "data = im.fromarray(cropped_image)\n",
        "data.save('cropped.jpg')\n",
        "\n",
        "#display image\n",
        "plt.imshow(cropped_image)"
      ],
      "metadata": {
        "id": "FY7o4ZegJGcc"
      },
      "execution_count": 67,
      "outputs": []
    }
  ]
}